{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9bbe9b8-b842-4595-9bb9-87404771b885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 12:14:50.056325: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-15 12:14:50.194676: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-15 12:14:50.385711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-15 12:14:50.583235: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-15 12:14:50.584324: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-15 12:14:50.906437: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-15 12:14:52.529760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523746ba-c1e3-4051-b8dd-09c3e815e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"http://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir=tf.keras.utils.get_file('flower_photos',origin=dataset_url, cache_dir='.',untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29d058e-6f28-4c22-bf7d-ce14feb76815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "data_dir=pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504c1508-4561-4e9c-9fe2-568c246fcabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_images_dict={\n",
    "    'roses':list(data_dir.glob('roses/*.jpg')),\n",
    "    'dandelion':list(data_dir.glob('dandelion/*.jpg')),\n",
    "    'daisy':list(data_dir.glob('daisy/*.jpg')),\n",
    "    'sunflowers':list(data_dir.glob('sunflowers/*.jpg')),\n",
    "    'tulips':list(data_dir.glob('tulips/*.jpg')),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8584c2a-9df6-47ea-a6f7-ad30ab4cd371",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_labels_dict={\n",
    "    'roses':0,\n",
    "    'dandelion':1,\n",
    "    'daisy':2,\n",
    "    'sunflowers':3,\n",
    "    'tulips':4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac31d6d-0d7c-4ecd-ac5f-a01e39f12f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 500, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(str(flowers_images_dict['roses'][0]))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd7922d-2c68-470e-bffd-c503218396be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = [],[]\n",
    "for flower_name,images in flowers_images_dict.items():\n",
    "    for image in images:\n",
    "        img =cv2.imread(str(image))\n",
    "        resized_img=cv2.resize(img,(180,180))\n",
    "        X.append(resized_img)\n",
    "        Y.append(flowers_labels_dict[flower_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5050d6-96c4-4c1e-840d-a6dac22dff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(X)\n",
    "Y=np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd4c7ee-b6d4-46a2-9fc9-b4ee35e7dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "627288eb-7913-4253-8f99-5b1599ccdccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled=X_train/255\n",
    "X_test_scaled=X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd430265-e5d1-4bfd-b457-4a3ca629fad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 1, 4, 0, 0, 3, 3, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b0091e8-87d9-40a4-91d4-7eaea407b793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iimorad31/.local/lib/python3.10/site-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - accuracy: 0.3398 - loss: 2.4629\n",
      "Epoch 2/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.5456 - loss: 1.1209\n",
      "Epoch 3/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.6119 - loss: 1.0137\n",
      "Epoch 4/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - accuracy: 0.6269 - loss: 0.9703\n",
      "Epoch 5/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.6513 - loss: 0.9058\n",
      "Epoch 6/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 1s/step - accuracy: 0.6673 - loss: 0.8927\n",
      "Epoch 7/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 1s/step - accuracy: 0.6904 - loss: 0.7900\n",
      "Epoch 8/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 1s/step - accuracy: 0.7249 - loss: 0.7601\n",
      "Epoch 9/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 1s/step - accuracy: 0.7046 - loss: 0.7520\n",
      "Epoch 10/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 1s/step - accuracy: 0.7501 - loss: 0.6727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f7e74a66830>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_augmentation = Sequential([\n",
    "    layers.RandomFlip(\"horizontal\",input_shape=(180, 180, 3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1)\n",
    "    \n",
    "])\n",
    "\n",
    "num_classes=5\n",
    "model = Sequential([\n",
    "    layers.Input((180, 180, 3)),\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')  # Add softmax\n",
    "])\n",
    "\n",
    "model.compile(loss= 'sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_scaled, Y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82e10746-f7df-4b35-be17-f52bfa760f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 265ms/step - accuracy: 0.7147 - loss: 0.7324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7805808782577515, 0.7113289833068848]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a0150-331d-4f15-9c77-ec03b2cdd82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
